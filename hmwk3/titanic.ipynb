{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Titanic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titanic = pd.read_csv(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['S' 'C' 'Q' nan]\n"
     ]
    }
   ],
   "source": [
    "# The titanic variable is available here.\n",
    "titanic.Age = titanic.Age.fillna(titanic.Age.median())\n",
    "# Replace all the occurences of male with the number 0.\n",
    "titanic.loc[titanic[\"Sex\"] == \"male\", \"Sex\"] = 0\n",
    "titanic.loc[titanic[\"Sex\"] == \"female\", \"Sex\"] = 1\n",
    "\n",
    "# Find all the unique values for \"Embarked\".\n",
    "print(titanic[\"Embarked\"].unique())\n",
    "titanic[\"Embarked\"] = titanic[\"Embarked\"].fillna(\"S\")\n",
    "\n",
    "titanic.loc[titanic[\"Embarked\"] == \"S\", \"Embarked\"] = 0\n",
    "titanic.loc[titanic[\"Embarked\"] == \"C\", \"Embarked\"] = 1\n",
    "titanic.loc[titanic[\"Embarked\"] == \"Q\", \"Embarked\"] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Feature Engineering\n",
    "\n",
    "We can also generate new features. Here are some ideas:\n",
    "\n",
    "The length of the name -- this could pertain to how rich the person was, and therefore their position in the Titanic.\n",
    "The total number of people in a family (SibSp + Parch).\n",
    "An easy way to generate features is to use the .apply method on pandas dataframes. This applies a function you pass in to each element in a dataframe or series. We can pass in a lambda function, which enables us to define a function inline.\n",
    "\n",
    "To write a lambda function, you write lambda x: len(x). x will take on the value of the input that is passed in -- in this case, the passenger name. The function to the right of the colon is then applied to x, and the result returned. The .apply method takes all of these outputs and constructs a pandas series from them. We can assign this series to a dataframe column.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generating a familysize column\n",
    "titanic[\"FamilySize\"] = titanic[\"SibSp\"] + titanic[\"Parch\"]\n",
    "\n",
    "# The .apply method generates a new series\n",
    "titanic[\"NameLength\"] = titanic[\"Name\"].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using The Title\n",
    "\n",
    "We can extract the title of the passenger from their name. The titles take the form of Master., Mr., Mrs.. There are a few very commonly used titles, and a \"long tail\" of one-off titles that only one or two passengers have.\n",
    "\n",
    "We'll first extract the titles with a regular expression, and then map each unique title to an integer value.\n",
    "\n",
    "We'll then have a numeric column that corresponds to the appropriate Title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr          517\n",
      "Miss        182\n",
      "Mrs         125\n",
      "Master       40\n",
      "Dr            7\n",
      "Rev           6\n",
      "Col           2\n",
      "Major         2\n",
      "Mlle          2\n",
      "Countess      1\n",
      "Ms            1\n",
      "Lady          1\n",
      "Jonkheer      1\n",
      "Don           1\n",
      "Mme           1\n",
      "Capt          1\n",
      "Sir           1\n",
      "Name: Name, dtype: int64\n",
      "1     517\n",
      "2     183\n",
      "3     125\n",
      "4      40\n",
      "5       7\n",
      "6       6\n",
      "7       5\n",
      "10      3\n",
      "8       3\n",
      "9       2\n",
      "Name: Name, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# A function to get the title from a name.\n",
    "def get_title(name):\n",
    "    # Use a regular expression to search for a title.  Titles always consist of capital and lowercase letters, and end with a period.\n",
    "    title_search = re.search(' ([A-Za-z]+)\\.', name)\n",
    "    # If the title exists, extract and return it.\n",
    "    if title_search:\n",
    "        return title_search.group(1)\n",
    "    return \"\"\n",
    "\n",
    "# Get all the titles and print how often each one occurs.\n",
    "titles = titanic[\"Name\"].apply(get_title)\n",
    "print(pd.value_counts(titles))\n",
    "\n",
    "# Map each title to an integer.  Some titles are very rare, and are compressed into the same codes as other titles.\n",
    "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Dr\": 5, \"Rev\": 6, \"Major\": 7, \"Col\": 7, \"Mlle\": 8, \"Mme\": 8, \"Don\": 9, \"Lady\": 10, \"Countess\": 10, \"Jonkheer\": 10, \"Sir\": 9, \"Capt\": 7, \"Ms\": 2}\n",
    "for k,v in title_mapping.items():\n",
    "    titles[titles == k] = v\n",
    "\n",
    "# Verify that we converted everything.\n",
    "print(pd.value_counts(titles))\n",
    "\n",
    "# Add in the title column.\n",
    "titanic[\"Title\"] = titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Family Groups\n",
    "\n",
    "We can also generate a feature indicating which family people are in. Because survival was likely highly dependent on your family and the people around you, this has a good chance at being a good feature.\n",
    "\n",
    "To get this, we'll concatenate someone's last name with FamilySize to get a unique family id. We'll then be able to assign a code to each person based on their family id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1      800\n",
      " 14       8\n",
      " 149      7\n",
      " 63       6\n",
      " 50       6\n",
      " 59       6\n",
      " 17       5\n",
      " 384      4\n",
      " 27       4\n",
      " 25       4\n",
      " 162      4\n",
      " 8        4\n",
      " 84       4\n",
      " 340      4\n",
      " 43       3\n",
      " 269      3\n",
      " 58       3\n",
      " 633      2\n",
      " 167      2\n",
      " 280      2\n",
      " 510      2\n",
      " 90       2\n",
      " 83       1\n",
      " 625      1\n",
      " 376      1\n",
      " 449      1\n",
      " 498      1\n",
      " 588      1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "# A dictionary mapping family name to id\n",
    "family_id_mapping = {}\n",
    "\n",
    "# A function to get the id given a row\n",
    "def get_family_id(row):\n",
    "    # Find the last name by splitting on a comma\n",
    "    last_name = row[\"Name\"].split(\",\")[0]\n",
    "    # Create the family id\n",
    "    family_id = \"{0}{1}\".format(last_name, row[\"FamilySize\"])\n",
    "    # Look up the id in the mapping\n",
    "    if family_id not in family_id_mapping:\n",
    "        if len(family_id_mapping) == 0:\n",
    "            current_id = 1\n",
    "        else:\n",
    "            # Get the maximum id from the mapping and add one to it if we don't have an id\n",
    "            current_id = (max(family_id_mapping.items(), key=operator.itemgetter(1))[1] + 1)\n",
    "        family_id_mapping[family_id] = current_id\n",
    "    return family_id_mapping[family_id]\n",
    "\n",
    "# Get the family ids with the apply method\n",
    "family_ids = titanic.apply(get_family_id, axis=1)\n",
    "\n",
    "# There are a lot of family ids, so we'll compress all of the families under 3 members into one code.\n",
    "family_ids[titanic[\"FamilySize\"] < 3] = -1\n",
    "\n",
    "# Print the count of each unique id.\n",
    "print(pd.value_counts(family_ids))\n",
    "\n",
    "titanic[\"FamilyId\"] = family_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "\n",
    "Feature engineering is the most important part of any machine learning task, and there are lots more features we could calculate. But we also need a way to figure out which features are the best.\n",
    "\n",
    "One way to do this is to use univariate feature selection. This essentially goes column by column, and figures out which columns correlate most closely with what we're trying to predict (Survived).\n",
    "\n",
    "As usual, sklearn has a function that will help us with feature selection, SelectKBest. This selects the best features from the data, and allows us to specify how many it selects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW0AAAEsCAYAAAAIBeLrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111af43d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\", \"FamilySize\", \"Title\", \"FamilyId\"]\n",
    "\n",
    "# Perform feature selection\n",
    "selector = SelectKBest(f_classif, k=5)\n",
    "selector.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "\n",
    "# Get the raw p-values for each feature, and transform from p-values into scores\n",
    "scores = -np.log10(selector.pvalues_)\n",
    "\n",
    "# Plot the scores.  See how \"Pclass\", \"Sex\", \"Title\", and \"Fare\" are the best?\n",
    "plt.bar(range(len(predictors)), scores)\n",
    "plt.xticks(range(len(predictors)), predictors, rotation='vertical')\n",
    "plt.show()\n",
    "\n",
    "# Pick only the four best features.\n",
    "predictors = [\"Pclass\", \"Sex\", \"Fare\", \"Title\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final preprocessed dataframe. Now it is ready for training and evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>FamilySize</th>\n",
       "      <th>NameLength</th>\n",
       "      <th>Title</th>\n",
       "      <th>FamilyId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name Sex  Age  SibSp  Parch  \\\n",
       "0                            Braund, Mr. Owen Harris   0   22      1      0   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...   1   38      1      0   \n",
       "2                             Heikkinen, Miss. Laina   1   26      0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)   1   35      1      0   \n",
       "4                           Allen, Mr. William Henry   0   35      0      0   \n",
       "\n",
       "             Ticket     Fare Cabin Embarked  FamilySize  NameLength Title  \\\n",
       "0         A/5 21171   7.2500   NaN        0           1          23     1   \n",
       "1          PC 17599  71.2833   C85        1           1          51     3   \n",
       "2  STON/O2. 3101282   7.9250   NaN        0           0          22     2   \n",
       "3            113803  53.1000  C123        0           1          44     3   \n",
       "4            373450   8.0500   NaN        0           0          24     1   \n",
       "\n",
       "   FamilyId  \n",
       "0        -1  \n",
       "1        -1  \n",
       "2        -1  \n",
       "3        -1  \n",
       "4        -1  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#My routine that encapsulates all the preprocessing work documented above\n",
    "def preprocess(data):\n",
    "    data[\"Age\"] = data[\"Age\"].fillna(data[\"Age\"].median())\n",
    "    data[\"Fare\"] = data[\"Fare\"].fillna(data[\"Fare\"].median())\n",
    "    data.loc[data[\"Sex\"] == \"male\", \"Sex\"] = 0 \n",
    "    data.loc[data[\"Sex\"] == \"female\", \"Sex\"] = 1\n",
    "    data[\"Embarked\"] = data[\"Embarked\"].fillna(\"S\")\n",
    "\n",
    "    data.loc[data[\"Embarked\"] == \"S\", \"Embarked\"] = 0\n",
    "    data.loc[data[\"Embarked\"] == \"C\", \"Embarked\"] = 1\n",
    "    data.loc[data[\"Embarked\"] == \"Q\", \"Embarked\"] = 2\n",
    "\n",
    "    # Generating a familysize column\n",
    "    data[\"FamilySize\"] = data[\"SibSp\"] + data[\"Parch\"]\n",
    "\n",
    "    # The .apply method generates a new series\n",
    "    data[\"NameLength\"] = data[\"Name\"].apply(lambda x: len(x))\n",
    "\n",
    "\n",
    "    import re\n",
    "    # Get all the titles and print how often each one occurs.\n",
    "    titles = data[\"Name\"].apply(get_title)\n",
    "    #     print(pd.value_counts(titles))\n",
    "\n",
    "    # Map each title to an integer.  Some titles are very rare, and are compressed into the same codes as other titles.\n",
    "    title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Dr\": 5, \"Rev\": 6, \"Major\": 7, \"Col\": 7, \"Mlle\": 8, \"Mme\": 8, \"Don\": 9, \"Lady\": 10, \"Countess\": 10, \"Jonkheer\": 10, \"Sir\": 9, \"Capt\": 7, \"Ms\": 2, \"Dona\": 10}\n",
    "    for k,v in title_mapping.items():\n",
    "        titles[titles == k] = v\n",
    "\n",
    "    # Verify that we converted everything.\n",
    "    #     print(pd.value_counts(titles))\n",
    "\n",
    "    # Add in the title column.\n",
    "    data[\"Title\"] = titles\n",
    "\n",
    "    import operator\n",
    "\n",
    "    # A dictionary mapping family name to id\n",
    "    family_id_mapping = {}\n",
    "\n",
    "    # Get the family ids with the apply method\n",
    "    family_ids = data.apply(get_family_id, axis=1)\n",
    "\n",
    "    # There are a lot of family ids, so we'll compress all of the families under 3 members into one code.\n",
    "    family_ids[data[\"FamilySize\"] < 3] = -1\n",
    "\n",
    "    # Print the count of each unique id.\n",
    "    #     print(pd.value_counts(family_ids))\n",
    "\n",
    "    data[\"FamilyId\"] = family_ids\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = titanic = pd.read_csv(\"data/train.csv\")\n",
    "titanic = preprocess(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import  grid_search\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on training set:\n",
      "\n",
      "{'max_features': None, 'max_depth': 9}\n",
      "Best parameters set found on training set:\n",
      "\n",
      "0.842873176207\n",
      "\n",
      "Grid scores on training set:\n",
      "\n",
      "0.712 (+/-0.171) for {'max_features': 'auto', 'max_depth': 1}\n",
      "0.756 (+/-0.141) for {'max_features': 'sqrt', 'max_depth': 1}\n",
      "0.774 (+/-0.094) for {'max_features': 'log2', 'max_depth': 1}\n",
      "0.782 (+/-0.066) for {'max_features': None, 'max_depth': 1}\n",
      "0.777 (+/-0.073) for {'max_features': 'auto', 'max_depth': 2}\n",
      "0.779 (+/-0.068) for {'max_features': 'sqrt', 'max_depth': 2}\n",
      "0.777 (+/-0.065) for {'max_features': 'log2', 'max_depth': 2}\n",
      "0.789 (+/-0.076) for {'max_features': None, 'max_depth': 2}\n",
      "0.797 (+/-0.082) for {'max_features': 'auto', 'max_depth': 3}\n",
      "0.783 (+/-0.073) for {'max_features': 'sqrt', 'max_depth': 3}\n",
      "0.793 (+/-0.068) for {'max_features': 'log2', 'max_depth': 3}\n",
      "0.825 (+/-0.063) for {'max_features': None, 'max_depth': 3}\n",
      "0.816 (+/-0.072) for {'max_features': 'auto', 'max_depth': 4}\n",
      "0.809 (+/-0.080) for {'max_features': 'sqrt', 'max_depth': 4}\n",
      "0.808 (+/-0.069) for {'max_features': 'log2', 'max_depth': 4}\n",
      "0.828 (+/-0.058) for {'max_features': None, 'max_depth': 4}\n",
      "0.811 (+/-0.071) for {'max_features': 'auto', 'max_depth': 5}\n",
      "0.807 (+/-0.074) for {'max_features': 'sqrt', 'max_depth': 5}\n",
      "0.813 (+/-0.052) for {'max_features': 'log2', 'max_depth': 5}\n",
      "0.817 (+/-0.080) for {'max_features': None, 'max_depth': 5}\n",
      "0.826 (+/-0.044) for {'max_features': 'auto', 'max_depth': 6}\n",
      "0.826 (+/-0.069) for {'max_features': 'sqrt', 'max_depth': 6}\n",
      "0.824 (+/-0.090) for {'max_features': 'log2', 'max_depth': 6}\n",
      "0.827 (+/-0.062) for {'max_features': None, 'max_depth': 6}\n",
      "0.817 (+/-0.095) for {'max_features': 'auto', 'max_depth': 7}\n",
      "0.824 (+/-0.088) for {'max_features': 'sqrt', 'max_depth': 7}\n",
      "0.833 (+/-0.061) for {'max_features': 'log2', 'max_depth': 7}\n",
      "0.831 (+/-0.063) for {'max_features': None, 'max_depth': 7}\n",
      "0.832 (+/-0.076) for {'max_features': 'auto', 'max_depth': 8}\n",
      "0.815 (+/-0.083) for {'max_features': 'sqrt', 'max_depth': 8}\n",
      "0.814 (+/-0.084) for {'max_features': 'log2', 'max_depth': 8}\n",
      "0.828 (+/-0.061) for {'max_features': None, 'max_depth': 8}\n",
      "0.824 (+/-0.048) for {'max_features': 'auto', 'max_depth': 9}\n",
      "0.818 (+/-0.071) for {'max_features': 'sqrt', 'max_depth': 9}\n",
      "0.818 (+/-0.069) for {'max_features': 'log2', 'max_depth': 9}\n",
      "0.843 (+/-0.051) for {'max_features': None, 'max_depth': 9}\n",
      "0.815 (+/-0.076) for {'max_features': 'auto', 'max_depth': 10}\n",
      "0.828 (+/-0.095) for {'max_features': 'sqrt', 'max_depth': 10}\n",
      "0.822 (+/-0.081) for {'max_features': 'log2', 'max_depth': 10}\n",
      "0.834 (+/-0.070) for {'max_features': None, 'max_depth': 10}\n",
      "0.810 (+/-0.062) for {'max_features': 'auto', 'max_depth': 11}\n",
      "0.817 (+/-0.065) for {'max_features': 'sqrt', 'max_depth': 11}\n",
      "0.813 (+/-0.071) for {'max_features': 'log2', 'max_depth': 11}\n",
      "0.829 (+/-0.063) for {'max_features': None, 'max_depth': 11}\n",
      "0.806 (+/-0.066) for {'max_features': 'auto', 'max_depth': 12}\n",
      "0.809 (+/-0.071) for {'max_features': 'sqrt', 'max_depth': 12}\n",
      "0.816 (+/-0.085) for {'max_features': 'log2', 'max_depth': 12}\n",
      "0.831 (+/-0.065) for {'max_features': None, 'max_depth': 12}\n",
      "0.818 (+/-0.066) for {'max_features': 'auto', 'max_depth': 13}\n",
      "0.823 (+/-0.067) for {'max_features': 'sqrt', 'max_depth': 13}\n",
      "0.808 (+/-0.080) for {'max_features': 'log2', 'max_depth': 13}\n",
      "0.831 (+/-0.057) for {'max_features': None, 'max_depth': 13}\n",
      "0.816 (+/-0.052) for {'max_features': 'auto', 'max_depth': 14}\n",
      "0.824 (+/-0.063) for {'max_features': 'sqrt', 'max_depth': 14}\n",
      "0.814 (+/-0.079) for {'max_features': 'log2', 'max_depth': 14}\n",
      "0.833 (+/-0.053) for {'max_features': None, 'max_depth': 14}\n",
      "0.814 (+/-0.068) for {'max_features': 'auto', 'max_depth': 15}\n",
      "0.815 (+/-0.077) for {'max_features': 'sqrt', 'max_depth': 15}\n",
      "0.815 (+/-0.068) for {'max_features': 'log2', 'max_depth': 15}\n",
      "0.824 (+/-0.056) for {'max_features': None, 'max_depth': 15}\n",
      "0.813 (+/-0.066) for {'max_features': 'auto', 'max_depth': 16}\n",
      "0.814 (+/-0.075) for {'max_features': 'sqrt', 'max_depth': 16}\n",
      "0.811 (+/-0.064) for {'max_features': 'log2', 'max_depth': 16}\n",
      "0.827 (+/-0.049) for {'max_features': None, 'max_depth': 16}\n",
      "0.813 (+/-0.059) for {'max_features': 'auto', 'max_depth': 17}\n",
      "0.816 (+/-0.057) for {'max_features': 'sqrt', 'max_depth': 17}\n",
      "0.808 (+/-0.055) for {'max_features': 'log2', 'max_depth': 17}\n",
      "0.826 (+/-0.048) for {'max_features': None, 'max_depth': 17}\n",
      "0.811 (+/-0.064) for {'max_features': 'auto', 'max_depth': 18}\n",
      "0.813 (+/-0.061) for {'max_features': 'sqrt', 'max_depth': 18}\n",
      "0.818 (+/-0.061) for {'max_features': 'log2', 'max_depth': 18}\n",
      "0.825 (+/-0.046) for {'max_features': None, 'max_depth': 18}\n",
      "0.813 (+/-0.061) for {'max_features': 'auto', 'max_depth': 19}\n",
      "0.814 (+/-0.058) for {'max_features': 'sqrt', 'max_depth': 19}\n",
      "0.814 (+/-0.061) for {'max_features': 'log2', 'max_depth': 19}\n",
      "0.826 (+/-0.048) for {'max_features': None, 'max_depth': 19}\n"
     ]
    }
   ],
   "source": [
    "parameters = {'max_depth':range(1,20), \n",
    "              'max_features': [\"auto\", \"sqrt\", \"log2\", None]\n",
    "             }\n",
    "dtgscv = grid_search.GridSearchCV(DecisionTreeClassifier(), parameters, cv=10, n_jobs=-1)\n",
    "dtgscv.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "dtree = dtgscv.best_estimator_\n",
    "\n",
    "print(\"Best parameters set found on training set:\")\n",
    "print\n",
    "print(dtgscv.best_params_)\n",
    "print(\"Best parameters set found on training set:\")\n",
    "print\n",
    "print(dtgscv.best_score_)\n",
    "print\n",
    "print(\"Grid scores on training set:\")\n",
    "print\n",
    "for params, mean_score, scores in dtgscv.grid_scores_:\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "          % (mean_score, scores.std() * 2, params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "#### Parameter Tuning\n",
    "\n",
    "The first, and easiest, thing we can do to improve the accuracy of the random forest is to increase the number of trees we're using. Training more trees will take more time, but because of the fact that we're averaging many predictions made on different subsets of the data, having more trees will increase accuracy greatly (up to a point).\n",
    "\n",
    "We can also tweak the min_samples_split and min_samples_leaf variables to reduce overfitting. Because of how a decision tree works (as we explained in the video), having splits that go all the way down, or overly deep in the tree can result in fitting to quirks in the dataset, and not true signal. Thus, increasing min_samples_split and min_samples_leaf can reduce overfitting, which will actually improve our score, as we're making predictions on unseen data. A model that is less overfit, and that can generalize better, will actually perform better on unseen data, but worse on seen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on training set:\n",
      "\n",
      "{'min_samples_split': 7, 'n_estimators': 150}\n",
      "\n",
      "Best parameters set found on training set:\n",
      "\n",
      "0.826038159371\n",
      "\n",
      "Grid scores on training set:\n",
      "\n",
      "0.817 (+/-0.059) for {'min_samples_split': 1, 'n_estimators': 100}\n",
      "0.816 (+/-0.060) for {'min_samples_split': 1, 'n_estimators': 150}\n",
      "0.818 (+/-0.059) for {'min_samples_split': 1, 'n_estimators': 200}\n",
      "0.814 (+/-0.070) for {'min_samples_split': 2, 'n_estimators': 100}\n",
      "0.819 (+/-0.062) for {'min_samples_split': 2, 'n_estimators': 150}\n",
      "0.817 (+/-0.057) for {'min_samples_split': 2, 'n_estimators': 200}\n",
      "0.820 (+/-0.060) for {'min_samples_split': 3, 'n_estimators': 100}\n",
      "0.820 (+/-0.053) for {'min_samples_split': 3, 'n_estimators': 150}\n",
      "0.822 (+/-0.060) for {'min_samples_split': 3, 'n_estimators': 200}\n",
      "0.820 (+/-0.059) for {'min_samples_split': 4, 'n_estimators': 100}\n",
      "0.822 (+/-0.065) for {'min_samples_split': 4, 'n_estimators': 150}\n",
      "0.819 (+/-0.051) for {'min_samples_split': 4, 'n_estimators': 200}\n",
      "0.824 (+/-0.057) for {'min_samples_split': 5, 'n_estimators': 100}\n",
      "0.820 (+/-0.055) for {'min_samples_split': 5, 'n_estimators': 150}\n",
      "0.819 (+/-0.058) for {'min_samples_split': 5, 'n_estimators': 200}\n",
      "0.819 (+/-0.065) for {'min_samples_split': 6, 'n_estimators': 100}\n",
      "0.822 (+/-0.056) for {'min_samples_split': 6, 'n_estimators': 150}\n",
      "0.819 (+/-0.061) for {'min_samples_split': 6, 'n_estimators': 200}\n",
      "0.824 (+/-0.064) for {'min_samples_split': 7, 'n_estimators': 100}\n",
      "0.826 (+/-0.061) for {'min_samples_split': 7, 'n_estimators': 150}\n",
      "0.823 (+/-0.064) for {'min_samples_split': 7, 'n_estimators': 200}\n",
      "0.819 (+/-0.074) for {'min_samples_split': 8, 'n_estimators': 100}\n",
      "0.823 (+/-0.063) for {'min_samples_split': 8, 'n_estimators': 150}\n",
      "0.819 (+/-0.061) for {'min_samples_split': 8, 'n_estimators': 200}\n",
      "0.823 (+/-0.072) for {'min_samples_split': 9, 'n_estimators': 100}\n",
      "0.820 (+/-0.059) for {'min_samples_split': 9, 'n_estimators': 150}\n",
      "0.822 (+/-0.065) for {'min_samples_split': 9, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "# alg = RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=4, min_samples_leaf=2)\n",
    "\n",
    "parameters = {\n",
    "    \"n_estimators\": [100, 150, 200], \n",
    "    \"min_samples_split\": range(1,10), \n",
    "#     \"min_samples_leaf\": range(1,10)\n",
    "    }\n",
    "rff_gscv = grid_search.GridSearchCV(RandomForestClassifier(), parameters, cv=10, n_jobs=-1)\n",
    "rff_gscv.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "rfc = rff_gscv.best_estimator_\n",
    "\n",
    "print(\"Best parameters set found on training set:\")\n",
    "print\n",
    "print(rff_gscv.best_params_)\n",
    "print\n",
    "print(\"Best parameters set found on training set:\")\n",
    "print\n",
    "print(rff_gscv.best_score_)\n",
    "print\n",
    "print(\"Grid scores on training set:\")\n",
    "print\n",
    "for params, mean_score, scores in rff_gscv.grid_scores_:\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "          % (mean_score, scores.std() * 2, params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Gradient Boosted\n",
    "\n",
    "Another method that builds on decision trees is a gradient boosting classifier. Boosting involves training decision trees one after another, and feeding the errors from one tree into the next tree. So each tree is building on all the other trees that came before it. This can lead to overfitting if we build too many trees, though. As you get above 100 trees or so, it's very easy to overfit and train to quirks in the dataset. As our dataset is extremely small, we'll limit the tree count to just 25.\n",
    "\n",
    "Another way to limit overfitting is to limit the depth to which each tree in the gradient boosting process can be built. We'll limit the tree depth to 3 to avoid overfitting.\n",
    "\n",
    "We'll try boosting instead of our random forest approach and see if we can improve our accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on training set:\n",
      "\n",
      "{'n_estimators': 100, 'learning_rate': 1, 'max_depth': 2}\n",
      "Best parameters set found on training set:\n",
      "\n",
      "0.821548821549\n",
      "\n",
      "Grid scores on training set:\n",
      "\n",
      "0.815 (+/-0.038) for {'n_estimators': 100, 'learning_rate': 1, 'max_depth': 1}\n",
      "0.809 (+/-0.033) for {'n_estimators': 150, 'learning_rate': 1, 'max_depth': 1}\n",
      "0.822 (+/-0.050) for {'n_estimators': 100, 'learning_rate': 1, 'max_depth': 2}\n",
      "0.817 (+/-0.073) for {'n_estimators': 150, 'learning_rate': 1, 'max_depth': 2}\n",
      "0.804 (+/-0.063) for {'n_estimators': 100, 'learning_rate': 1, 'max_depth': 3}\n",
      "0.813 (+/-0.071) for {'n_estimators': 150, 'learning_rate': 1, 'max_depth': 3}\n",
      "0.802 (+/-0.070) for {'n_estimators': 100, 'learning_rate': 1, 'max_depth': 4}\n",
      "0.808 (+/-0.066) for {'n_estimators': 150, 'learning_rate': 1, 'max_depth': 4}\n",
      "0.791 (+/-0.074) for {'n_estimators': 100, 'learning_rate': 1, 'max_depth': 5}\n",
      "0.796 (+/-0.069) for {'n_estimators': 150, 'learning_rate': 1, 'max_depth': 5}\n",
      "0.804 (+/-0.064) for {'n_estimators': 100, 'learning_rate': 1, 'max_depth': 6}\n",
      "0.797 (+/-0.066) for {'n_estimators': 150, 'learning_rate': 1, 'max_depth': 6}\n",
      "0.802 (+/-0.064) for {'n_estimators': 100, 'learning_rate': 1, 'max_depth': 7}\n",
      "0.809 (+/-0.082) for {'n_estimators': 150, 'learning_rate': 1, 'max_depth': 7}\n",
      "0.805 (+/-0.069) for {'n_estimators': 100, 'learning_rate': 1, 'max_depth': 8}\n",
      "0.804 (+/-0.063) for {'n_estimators': 150, 'learning_rate': 1, 'max_depth': 8}\n",
      "0.800 (+/-0.073) for {'n_estimators': 100, 'learning_rate': 1, 'max_depth': 9}\n",
      "0.800 (+/-0.070) for {'n_estimators': 150, 'learning_rate': 1, 'max_depth': 9}\n",
      "0.477 (+/-0.394) for {'n_estimators': 100, 'learning_rate': 2, 'max_depth': 1}\n",
      "0.477 (+/-0.394) for {'n_estimators': 150, 'learning_rate': 2, 'max_depth': 1}\n",
      "0.676 (+/-0.116) for {'n_estimators': 100, 'learning_rate': 2, 'max_depth': 2}\n",
      "0.676 (+/-0.116) for {'n_estimators': 150, 'learning_rate': 2, 'max_depth': 2}\n",
      "0.560 (+/-0.319) for {'n_estimators': 100, 'learning_rate': 2, 'max_depth': 3}\n",
      "0.560 (+/-0.319) for {'n_estimators': 150, 'learning_rate': 2, 'max_depth': 3}\n",
      "0.539 (+/-0.243) for {'n_estimators': 100, 'learning_rate': 2, 'max_depth': 4}\n",
      "0.539 (+/-0.243) for {'n_estimators': 150, 'learning_rate': 2, 'max_depth': 4}\n",
      "0.604 (+/-0.139) for {'n_estimators': 100, 'learning_rate': 2, 'max_depth': 5}\n",
      "0.604 (+/-0.139) for {'n_estimators': 150, 'learning_rate': 2, 'max_depth': 5}\n",
      "0.510 (+/-0.032) for {'n_estimators': 100, 'learning_rate': 2, 'max_depth': 6}\n",
      "0.501 (+/-0.114) for {'n_estimators': 150, 'learning_rate': 2, 'max_depth': 6}\n",
      "0.550 (+/-0.113) for {'n_estimators': 100, 'learning_rate': 2, 'max_depth': 7}\n",
      "0.581 (+/-0.064) for {'n_estimators': 150, 'learning_rate': 2, 'max_depth': 7}\n",
      "0.642 (+/-0.088) for {'n_estimators': 100, 'learning_rate': 2, 'max_depth': 8}\n",
      "0.641 (+/-0.086) for {'n_estimators': 150, 'learning_rate': 2, 'max_depth': 8}\n",
      "0.631 (+/-0.112) for {'n_estimators': 100, 'learning_rate': 2, 'max_depth': 9}\n",
      "0.630 (+/-0.110) for {'n_estimators': 150, 'learning_rate': 2, 'max_depth': 9}\n",
      "0.639 (+/-0.360) for {'n_estimators': 100, 'learning_rate': 3, 'max_depth': 1}\n",
      "0.639 (+/-0.360) for {'n_estimators': 150, 'learning_rate': 3, 'max_depth': 1}\n",
      "0.556 (+/-0.288) for {'n_estimators': 100, 'learning_rate': 3, 'max_depth': 2}\n",
      "0.556 (+/-0.288) for {'n_estimators': 150, 'learning_rate': 3, 'max_depth': 2}\n",
      "0.606 (+/-0.326) for {'n_estimators': 100, 'learning_rate': 3, 'max_depth': 3}\n",
      "0.606 (+/-0.326) for {'n_estimators': 150, 'learning_rate': 3, 'max_depth': 3}\n",
      "0.328 (+/-0.212) for {'n_estimators': 100, 'learning_rate': 3, 'max_depth': 4}\n",
      "0.328 (+/-0.212) for {'n_estimators': 150, 'learning_rate': 3, 'max_depth': 4}\n",
      "0.464 (+/-0.206) for {'n_estimators': 100, 'learning_rate': 3, 'max_depth': 5}\n",
      "0.464 (+/-0.206) for {'n_estimators': 150, 'learning_rate': 3, 'max_depth': 5}\n",
      "0.587 (+/-0.174) for {'n_estimators': 100, 'learning_rate': 3, 'max_depth': 6}\n",
      "0.596 (+/-0.149) for {'n_estimators': 150, 'learning_rate': 3, 'max_depth': 6}\n",
      "0.536 (+/-0.073) for {'n_estimators': 100, 'learning_rate': 3, 'max_depth': 7}\n",
      "0.536 (+/-0.073) for {'n_estimators': 150, 'learning_rate': 3, 'max_depth': 7}\n",
      "0.608 (+/-0.008) for {'n_estimators': 100, 'learning_rate': 3, 'max_depth': 8}\n",
      "0.608 (+/-0.008) for {'n_estimators': 150, 'learning_rate': 3, 'max_depth': 8}\n",
      "0.687 (+/-0.054) for {'n_estimators': 100, 'learning_rate': 3, 'max_depth': 9}\n",
      "0.691 (+/-0.052) for {'n_estimators': 150, 'learning_rate': 3, 'max_depth': 9}\n",
      "0.652 (+/-0.380) for {'n_estimators': 100, 'learning_rate': 4, 'max_depth': 1}\n",
      "0.652 (+/-0.380) for {'n_estimators': 150, 'learning_rate': 4, 'max_depth': 1}\n",
      "0.431 (+/-0.483) for {'n_estimators': 100, 'learning_rate': 4, 'max_depth': 2}\n",
      "0.431 (+/-0.483) for {'n_estimators': 150, 'learning_rate': 4, 'max_depth': 2}\n",
      "0.608 (+/-0.242) for {'n_estimators': 100, 'learning_rate': 4, 'max_depth': 3}\n",
      "0.608 (+/-0.242) for {'n_estimators': 150, 'learning_rate': 4, 'max_depth': 3}\n",
      "0.460 (+/-0.392) for {'n_estimators': 100, 'learning_rate': 4, 'max_depth': 4}\n",
      "0.460 (+/-0.392) for {'n_estimators': 150, 'learning_rate': 4, 'max_depth': 4}\n",
      "0.404 (+/-0.020) for {'n_estimators': 100, 'learning_rate': 4, 'max_depth': 5}\n",
      "0.404 (+/-0.020) for {'n_estimators': 150, 'learning_rate': 4, 'max_depth': 5}\n",
      "0.489 (+/-0.165) for {'n_estimators': 100, 'learning_rate': 4, 'max_depth': 6}\n",
      "0.502 (+/-0.150) for {'n_estimators': 150, 'learning_rate': 4, 'max_depth': 6}\n",
      "0.496 (+/-0.065) for {'n_estimators': 100, 'learning_rate': 4, 'max_depth': 7}\n",
      "0.495 (+/-0.067) for {'n_estimators': 150, 'learning_rate': 4, 'max_depth': 7}\n",
      "0.519 (+/-0.065) for {'n_estimators': 100, 'learning_rate': 4, 'max_depth': 8}\n",
      "0.517 (+/-0.066) for {'n_estimators': 150, 'learning_rate': 4, 'max_depth': 8}\n",
      "0.605 (+/-0.040) for {'n_estimators': 100, 'learning_rate': 4, 'max_depth': 9}\n",
      "0.611 (+/-0.051) for {'n_estimators': 150, 'learning_rate': 4, 'max_depth': 9}\n",
      "0.222 (+/-0.034) for {'n_estimators': 100, 'learning_rate': 5, 'max_depth': 1}\n",
      "0.222 (+/-0.034) for {'n_estimators': 150, 'learning_rate': 5, 'max_depth': 1}\n",
      "0.385 (+/-0.461) for {'n_estimators': 100, 'learning_rate': 5, 'max_depth': 2}\n",
      "0.385 (+/-0.461) for {'n_estimators': 150, 'learning_rate': 5, 'max_depth': 2}\n",
      "0.311 (+/-0.127) for {'n_estimators': 100, 'learning_rate': 5, 'max_depth': 3}\n",
      "0.311 (+/-0.127) for {'n_estimators': 150, 'learning_rate': 5, 'max_depth': 3}\n",
      "0.375 (+/-0.041) for {'n_estimators': 100, 'learning_rate': 5, 'max_depth': 4}\n",
      "0.375 (+/-0.041) for {'n_estimators': 150, 'learning_rate': 5, 'max_depth': 4}\n",
      "0.464 (+/-0.430) for {'n_estimators': 100, 'learning_rate': 5, 'max_depth': 5}\n",
      "0.464 (+/-0.430) for {'n_estimators': 150, 'learning_rate': 5, 'max_depth': 5}\n",
      "0.401 (+/-0.097) for {'n_estimators': 100, 'learning_rate': 5, 'max_depth': 6}\n",
      "0.421 (+/-0.068) for {'n_estimators': 150, 'learning_rate': 5, 'max_depth': 6}\n",
      "0.507 (+/-0.091) for {'n_estimators': 100, 'learning_rate': 5, 'max_depth': 7}\n",
      "0.507 (+/-0.089) for {'n_estimators': 150, 'learning_rate': 5, 'max_depth': 7}\n",
      "0.565 (+/-0.140) for {'n_estimators': 100, 'learning_rate': 5, 'max_depth': 8}\n",
      "0.561 (+/-0.144) for {'n_estimators': 150, 'learning_rate': 5, 'max_depth': 8}\n",
      "0.570 (+/-0.053) for {'n_estimators': 100, 'learning_rate': 5, 'max_depth': 9}\n",
      "0.570 (+/-0.051) for {'n_estimators': 150, 'learning_rate': 5, 'max_depth': 9}\n",
      "0.222 (+/-0.034) for {'n_estimators': 100, 'learning_rate': 6, 'max_depth': 1}\n",
      "0.222 (+/-0.034) for {'n_estimators': 150, 'learning_rate': 6, 'max_depth': 1}\n",
      "0.352 (+/-0.365) for {'n_estimators': 100, 'learning_rate': 6, 'max_depth': 2}\n",
      "0.352 (+/-0.365) for {'n_estimators': 150, 'learning_rate': 6, 'max_depth': 2}\n",
      "0.468 (+/-0.300) for {'n_estimators': 100, 'learning_rate': 6, 'max_depth': 3}\n",
      "0.468 (+/-0.300) for {'n_estimators': 150, 'learning_rate': 6, 'max_depth': 3}\n",
      "0.374 (+/-0.040) for {'n_estimators': 100, 'learning_rate': 6, 'max_depth': 4}\n",
      "0.374 (+/-0.040) for {'n_estimators': 150, 'learning_rate': 6, 'max_depth': 4}\n",
      "0.412 (+/-0.036) for {'n_estimators': 100, 'learning_rate': 6, 'max_depth': 5}\n",
      "0.412 (+/-0.036) for {'n_estimators': 150, 'learning_rate': 6, 'max_depth': 5}\n",
      "0.448 (+/-0.065) for {'n_estimators': 100, 'learning_rate': 6, 'max_depth': 6}\n",
      "0.455 (+/-0.047) for {'n_estimators': 150, 'learning_rate': 6, 'max_depth': 6}\n",
      "0.484 (+/-0.050) for {'n_estimators': 100, 'learning_rate': 6, 'max_depth': 7}\n",
      "0.486 (+/-0.047) for {'n_estimators': 150, 'learning_rate': 6, 'max_depth': 7}\n",
      "0.544 (+/-0.118) for {'n_estimators': 100, 'learning_rate': 6, 'max_depth': 8}\n",
      "0.543 (+/-0.119) for {'n_estimators': 150, 'learning_rate': 6, 'max_depth': 8}\n",
      "0.591 (+/-0.030) for {'n_estimators': 100, 'learning_rate': 6, 'max_depth': 9}\n",
      "0.591 (+/-0.030) for {'n_estimators': 150, 'learning_rate': 6, 'max_depth': 9}\n",
      "0.222 (+/-0.034) for {'n_estimators': 100, 'learning_rate': 7, 'max_depth': 1}\n",
      "0.222 (+/-0.034) for {'n_estimators': 150, 'learning_rate': 7, 'max_depth': 1}\n",
      "0.270 (+/-0.158) for {'n_estimators': 100, 'learning_rate': 7, 'max_depth': 2}\n",
      "0.270 (+/-0.158) for {'n_estimators': 150, 'learning_rate': 7, 'max_depth': 2}\n",
      "0.416 (+/-0.262) for {'n_estimators': 100, 'learning_rate': 7, 'max_depth': 3}\n",
      "0.416 (+/-0.262) for {'n_estimators': 150, 'learning_rate': 7, 'max_depth': 3}\n",
      "0.416 (+/-0.100) for {'n_estimators': 100, 'learning_rate': 7, 'max_depth': 4}\n",
      "0.416 (+/-0.100) for {'n_estimators': 150, 'learning_rate': 7, 'max_depth': 4}\n",
      "0.505 (+/-0.367) for {'n_estimators': 100, 'learning_rate': 7, 'max_depth': 5}\n",
      "0.505 (+/-0.367) for {'n_estimators': 150, 'learning_rate': 7, 'max_depth': 5}\n",
      "0.411 (+/-0.088) for {'n_estimators': 100, 'learning_rate': 7, 'max_depth': 6}\n",
      "0.410 (+/-0.086) for {'n_estimators': 150, 'learning_rate': 7, 'max_depth': 6}\n",
      "0.440 (+/-0.014) for {'n_estimators': 100, 'learning_rate': 7, 'max_depth': 7}\n",
      "0.452 (+/-0.051) for {'n_estimators': 150, 'learning_rate': 7, 'max_depth': 7}\n",
      "0.539 (+/-0.083) for {'n_estimators': 100, 'learning_rate': 7, 'max_depth': 8}\n",
      "0.542 (+/-0.074) for {'n_estimators': 150, 'learning_rate': 7, 'max_depth': 8}\n",
      "0.586 (+/-0.062) for {'n_estimators': 100, 'learning_rate': 7, 'max_depth': 9}\n",
      "0.580 (+/-0.073) for {'n_estimators': 150, 'learning_rate': 7, 'max_depth': 9}\n",
      "0.222 (+/-0.034) for {'n_estimators': 100, 'learning_rate': 8, 'max_depth': 1}\n",
      "0.222 (+/-0.034) for {'n_estimators': 150, 'learning_rate': 8, 'max_depth': 1}\n",
      "0.270 (+/-0.158) for {'n_estimators': 100, 'learning_rate': 8, 'max_depth': 2}\n",
      "0.270 (+/-0.158) for {'n_estimators': 150, 'learning_rate': 8, 'max_depth': 2}\n",
      "0.426 (+/-0.253) for {'n_estimators': 100, 'learning_rate': 8, 'max_depth': 3}\n",
      "0.426 (+/-0.253) for {'n_estimators': 150, 'learning_rate': 8, 'max_depth': 3}\n",
      "0.382 (+/-0.094) for {'n_estimators': 100, 'learning_rate': 8, 'max_depth': 4}\n",
      "0.382 (+/-0.094) for {'n_estimators': 150, 'learning_rate': 8, 'max_depth': 4}\n",
      "0.519 (+/-0.310) for {'n_estimators': 100, 'learning_rate': 8, 'max_depth': 5}\n",
      "0.519 (+/-0.310) for {'n_estimators': 150, 'learning_rate': 8, 'max_depth': 5}\n",
      "0.420 (+/-0.105) for {'n_estimators': 100, 'learning_rate': 8, 'max_depth': 6}\n",
      "0.418 (+/-0.104) for {'n_estimators': 150, 'learning_rate': 8, 'max_depth': 6}\n",
      "0.483 (+/-0.067) for {'n_estimators': 100, 'learning_rate': 8, 'max_depth': 7}\n",
      "0.484 (+/-0.070) for {'n_estimators': 150, 'learning_rate': 8, 'max_depth': 7}\n",
      "0.517 (+/-0.050) for {'n_estimators': 100, 'learning_rate': 8, 'max_depth': 8}\n",
      "0.522 (+/-0.044) for {'n_estimators': 150, 'learning_rate': 8, 'max_depth': 8}\n",
      "0.588 (+/-0.054) for {'n_estimators': 100, 'learning_rate': 8, 'max_depth': 9}\n",
      "0.580 (+/-0.063) for {'n_estimators': 150, 'learning_rate': 8, 'max_depth': 9}\n",
      "0.222 (+/-0.034) for {'n_estimators': 100, 'learning_rate': 9, 'max_depth': 1}\n",
      "0.222 (+/-0.034) for {'n_estimators': 150, 'learning_rate': 9, 'max_depth': 1}\n",
      "0.273 (+/-0.164) for {'n_estimators': 100, 'learning_rate': 9, 'max_depth': 2}\n",
      "0.273 (+/-0.164) for {'n_estimators': 150, 'learning_rate': 9, 'max_depth': 2}\n",
      "0.283 (+/-0.104) for {'n_estimators': 100, 'learning_rate': 9, 'max_depth': 3}\n",
      "0.283 (+/-0.104) for {'n_estimators': 150, 'learning_rate': 9, 'max_depth': 3}\n",
      "0.438 (+/-0.367) for {'n_estimators': 100, 'learning_rate': 9, 'max_depth': 4}\n",
      "0.438 (+/-0.367) for {'n_estimators': 150, 'learning_rate': 9, 'max_depth': 4}\n",
      "0.485 (+/-0.368) for {'n_estimators': 100, 'learning_rate': 9, 'max_depth': 5}\n",
      "0.484 (+/-0.371) for {'n_estimators': 150, 'learning_rate': 9, 'max_depth': 5}\n",
      "0.442 (+/-0.146) for {'n_estimators': 100, 'learning_rate': 9, 'max_depth': 6}\n",
      "0.442 (+/-0.146) for {'n_estimators': 150, 'learning_rate': 9, 'max_depth': 6}\n",
      "0.471 (+/-0.136) for {'n_estimators': 100, 'learning_rate': 9, 'max_depth': 7}\n",
      "0.456 (+/-0.097) for {'n_estimators': 150, 'learning_rate': 9, 'max_depth': 7}\n",
      "0.508 (+/-0.029) for {'n_estimators': 100, 'learning_rate': 9, 'max_depth': 8}\n",
      "0.505 (+/-0.031) for {'n_estimators': 150, 'learning_rate': 9, 'max_depth': 8}\n",
      "0.588 (+/-0.059) for {'n_estimators': 100, 'learning_rate': 9, 'max_depth': 9}\n",
      "0.589 (+/-0.062) for {'n_estimators': 150, 'learning_rate': 9, 'max_depth': 9}\n"
     ]
    }
   ],
   "source": [
    "# alg = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)\n",
    "\n",
    "parameters = {\n",
    "    \"learning_rate\": range(1,10), \n",
    "    \"n_estimators\": [ 100, 150], \n",
    "    \"max_depth\": range(1,10)\n",
    "    }\n",
    "gbc_gscv = grid_search.GridSearchCV(GradientBoostingClassifier(), parameters, cv=3, n_jobs = -1)\n",
    "gbc_gscv.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "gbc = gbc_gscv.best_estimator_\n",
    "\n",
    "print(\"Best parameters set found on training set:\")\n",
    "print\n",
    "print(gbc_gscv.best_params_)\n",
    "print(\"Best parameters set found on training set:\")\n",
    "print\n",
    "print(gbc_gscv.best_score_)\n",
    "print\n",
    "print(\"Grid scores on training set:\")\n",
    "print\n",
    "for params, mean_score, scores in gbc_gscv.grid_scores_:\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "          % (mean_score, scores.std() * 2, params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Kaggle Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"data/test.csv\")\n",
    "titanic_test = preprocess(test_data.copy())\n",
    "\n",
    "models = [dtree, rfc, gbc]\n",
    "\n",
    "for model in models:\n",
    "    # Train the algorithm using all the training data\n",
    "    model.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "\n",
    "    # Make predictions using the test set.\n",
    "    predictions = model.predict(titanic_test[predictors])\n",
    "\n",
    "    # Create a new dataframe with only the columns Kaggle wants from the dataset.\n",
    "    submission = pd.DataFrame({\n",
    "            \"PassengerId\": titanic_test[\"PassengerId\"],\n",
    "            \"Survived\": predictions\n",
    "        })\n",
    "    submission.to_csv(\"%s.csv\"%model.__class__.__name__, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
